# Write your short answers in this file, replacing the placeholders as appropriate.
# This assignment consists of 3 parts for a total of 79 points.
# For numerical answers, copy and paste at least 5 significant figures.
# - Neural Network DAN Text Classification (22 points)
# - WAN Text Classification (17 points)
# - BERT Text Classification (36 points)
# - Correct submission (2 point)
# - Answer file parses (2 point)



###################################################################
###################################################################
## Neural Network DAN Text Classification (22 points)
###################################################################
###################################################################


# ------------------------------------------------------------------
# | Section (1): Keras Functional API warm up (5 points)  | 
# ------------------------------------------------------------------

# Question 1.a (/5): I created a model using the Keras functional API that identically reproduces the model summary shown
# (This question is multiple choice.  Delete all but the correct answer).
neural_network_dan_text_classification_1_1_a: 
 - True


# ------------------------------------------------------------------
# | Section (1.1): Classification with various Word2Vec-based Models (2 points)  | 
# ------------------------------------------------------------------

# Question 1.1.a (/1): What is the percentage of positive examples in the training set (e.g. 72.575% is 0.72575)?
neural_network_dan_text_classification_1_1_1_1_a: 0.00000

# Question 1.1.b (/1): What is the percentage of positive examples in the test set (e.g. 72.575% is 0.72575)?
neural_network_dan_text_classification_1_1_1_1_b: 0.00000


# ------------------------------------------------------------------
# | Section (1.2): The Role of Shuffling of the Training Set (6 points)  | 
# ------------------------------------------------------------------

# Question 1.2.a (/3): What is the final validation accuracy that you observed after 10 epochs? (Copy and paste the decimal value e.g. a number like 0.5678 or 0.87654)
neural_network_dan_text_classification_1_2_1_2_a: 1.00000

# Question 1.2.b (/3): What is the final validation accuracy that you observed for the shuffled run after 10 epochs?
neural_network_dan_text_classification_1_2_1_2_b: 0.59050


# ------------------------------------------------------------------
# | Section (1.3): Approaches for Training of Embeddings (9 points)  | 
# ------------------------------------------------------------------

# Question 1.3.a (/3): What is the final validation accuracy that you observed for the static model after 10 epochs? (Copy and paste the decimal value e.g. a number like 0.5678 or 0.87654)
neural_network_dan_text_classification_1_3_1_3_a: 0.7575

# Question 1.3.b (/3): What is the final validation accuracy that you observed for the model where you initialized with word2vec vectors but allow them to retrain for 3 epochs? (Copy and paste the decimal value e.g. a number like 0.5678 or 0.87654)
neural_network_dan_text_classification_1_3_1_3_b:  0.78825

# Question 1.3.c (/3): What is the final validation accuracy that you observed for the model where you initialized randomly and then trained? (Copy and paste the decimal value e.g. a number like 0.5678 or 0.87654)
neural_network_dan_text_classification_1_3_1_3_c: 0.81000



###################################################################
###################################################################
## WAN Text Classification (17 points)
###################################################################
###################################################################


# ------------------------------------------------------------------
# | Section (2): Weighted Averaging Models using Attention (17 points)  | 
# ------------------------------------------------------------------

# Question 2.1.a (/2): Calculate the context vector for the following query and key/value vectors.
wan_text_classification_2_2_1_a: [0, 0.5, -1]

# Question 2.1.b (/2): What are the weights for the key/value vectors?
wan_text_classification_2_2_1_b: [0.5, 0.5]

# Question 2.2.a (/7): What is the final validation accuracy that you observed for the wan training after 10 epochs? (Copy and paste the decimal value e.g. a number like 0.5678 or 0.87654)
wan_text_classification_2_2_2_a: 0.77820

# Question 2.2.b (/3): List the 5 most important words separated by commas. (Again, if a word appears twice, include it twice.)
wan_text_classification_2_2_2_b: [but, am, when, good, phony]

# Question 2.2.c (/3): List the 5 least important words separated by commas. (Again, if a word appears twice, include it twice.)
wan_text_classification_2_2_2_c: [sit, role, us, psuedolove, i]



###################################################################
###################################################################
## BERT Text Classification (36 points)
###################################################################
###################################################################


# ------------------------------------------------------------------
# | Section (3.1): Tokenization with BERT (15 points)  | 
# ------------------------------------------------------------------

# Question 3.1.a (/1): Why do the attention_masks have 4 and 1 zeros, respectively?
# (This question is multiple choice.  Delete all but the correct answer).
bert_text_classification_3_1_3_1_a: 
 - For the first example 4 positions are padded while for the second one it is only one.

# Question 3.1.b (/1): How many outputs are there?
bert_text_classification_3_1_3_1_b: 
- The output consists of 2 sets of 10 vectors, each containing 768 values, corresponding to the token embeddings for both sentences.

# Question 3.1.c (/1): Which output do we need to use to get token-level embeddings?
# (This question is multiple choice.  Delete all but the correct answer).
bert_text_classification_3_1_3_1_c: 
 - the first


# Question 3.1.d (/2): Which input_id number corresponds to 'bank' in the two sentences?
bert_text_classification_3_1_3_1_d: 
- The word "bank" corresponds to the vocabulary ID 3085 in both sentences.

# Question 3.1.e (/2): Which token array index number corresponds to 'bank' in the first sentence?
bert_text_classification_3_1_3_1_e: 
- The token "bank" is at position 7 in the tokenized input of the first sentence.

# Question 3.1.f (/2): Which array index number corresponds to 'bank' in the second sentence?
bert_text_classification_3_1_3_1_f: 
- The token "bank" is at position 6 in the tokenized input of the second sentence.

# Question 3.1.g (/3): What is the cosine similarity between the BERT outputs for the two occurences of 'bank' in the two sentences?
bert_text_classification_3_1_3_1_g: 
- The cosine similarity between the two embeddings for 'bank': 0.14729

# Question 3.1.h (/3): How does this relate to the cosine similarity of 'this' (sentence 1) and 'the' (sentence 2). Compute the cosine similarity.
bert_text_classification_3_1_3_1_h: 
- The cosine similarity between 'this' and 'the': 0.89538. The cosine similarity between "this" (in sentence 1) and "the" (in sentence 2) is 
likely to be low because they are function words with different contextual embeddings. 
This would contrast with the cosine similarity of a word like "bank", which may be more similar across contexts due to its role as a content word.


# ------------------------------------------------------------------
# | Section (3.2): Classification with BERT (21 points)  | 
# ------------------------------------------------------------------

# Question 3.2.a (/7): What is the final validation accuracy that you observed for the BERT [CLS]-classification model after training for 2 epochs? (Copy and paste the decimal value e.g. a number like 0.5678 or 0.87654)
bert_text_classification_3_2_3_2_a: 0.65000

# Question 3.3.a (/7): What is the final validation accuracy that you observed for the BERT-averaging-classification model after training for 2 epochs? (Copy and paste the decimal value e.g. a number like 0.5678 or 0.87654)
bert_text_classification_3_2_3_3_a: 0.65000

# Question 3.4.a (/7): What is the final validation accuracy that you observed for the BERT-CNN-classification model after 2 epochs? (Copy and paste the decimal value e.g. a number like 0.5678 or 0.87654)
bert_text_classification_3_2_3_4_a: 0.50000
